{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nazmul1932/SPL_3/blob/main/spl3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Main Duplicate Detection App***"
      ],
      "metadata": {
        "id": "o3x7NJFctOtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask-ngrok\n",
        "!pip install pyngrok\n",
        "!pip install glove-python-binary"
      ],
      "metadata": {
        "id": "7VcMkLFDCmfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2IURrwCxP3fRHNWe4X0HczlIklw_4UqSNQMP8endQyki5TXpp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dC1VQMpFv75-",
        "outputId": "1d665177-2313-41cf-cd0d-e29959148ad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTMtumHU6hDr",
        "outputId": "f9b103b0-1163-46ed-a724-1bc3f5233467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, request, render_template\n",
        "import pickle\n",
        "from gensim.utils import simple_preprocess as SIMPLE_PREPROCESS  \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.parsing.preprocessing import STOPWORDS as STOP_WORDS\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from flask import jsonify\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.spatial import distance\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "from glove import Glove, Corpus\n",
        "from gensim.models import Word2Vec,FastText\n",
        "from gensim.test.utils import get_tmpfile\n",
        "from glove import Glove, Corpus\n",
        "import glove"
      ],
      "metadata": {
        "id": "A1lHikEyN7sj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13229440-e8c3-4301-c77f-df79d9e381af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanTextFromDescriptionAndTitle(report):\n",
        "\n",
        "  report = report.replace('\\t', '')\n",
        "  report = re.sub('\\w*\\d\\w*', '', report)\n",
        "  report = report.lower()\n",
        "  report = report.translate(str.maketrans('', '', string.punctuation))\n",
        "    \n",
        "  return report"
      ],
      "metadata": {
        "id": "pat_8EucOVmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wordTokenizationAndLemmatization(report):\n",
        "\n",
        "    new_report = []\n",
        "    for token in SIMPLE_PREPROCESS(report):\n",
        "        if token not in STOP_WORDS:\n",
        "          if len(token) > 5:\n",
        "            new_report.append(WordNetLemmatizer().lemmatize(token, pos='v'))\n",
        "    return new_report"
      ],
      "metadata": {
        "id": "GGEDCfRshbQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for c in range(10):\n",
        "  exec('topic{} = pd.read_csv(\"/content/drive/MyDrive/topic{}.csv\", error_bad_lines=False, engine=\"python\")'.format(c,c))\n",
        "  exec(\"topic{}= topic{}.drop(columns=['Unnamed: 0'])\".format(c,c))\n",
        "  exec(\"topic{}['Description'] = topic{}['Description'].map(wordTokenizationAndLemmatization)\".format(c,c))"
      ],
      "metadata": {
        "id": "I0b4tm3GV8As"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  exec('sent{} = []'.format(i))\n",
        "  exec('x = topic{}'.format(i))\n",
        "  for j in range(len(x)):\n",
        "    exec('sent{}.append(topic{}.Description[{}])'.format(i,i,j))"
      ],
      "metadata": {
        "id": "OqLm_zZbV-Xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sim_with_clusters_lda_topn(DR, n):\n",
        "    vec_bow = dictionary.doc2bow(DR)\n",
        "    x= ldamallet[vec_bow]\n",
        "    topic = np.asarray(x)\n",
        "    sim=[]\n",
        "    x= topic[np.argsort(topic[:,1])[-n:][::-1],0]\n",
        "    for i in range(len(x)):\n",
        "        sim.append(int(x[i]))\n",
        "    return sim"
      ],
      "metadata": {
        "id": "2IfKCJNfPPF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def average_word_vectors_ft(words, model, vocabulary, num_features):\n",
        "    \n",
        "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
        "    nwords = 0.\n",
        "    \n",
        "    for word in words:\n",
        "        if word in vocabulary: \n",
        "            nwords = nwords + 1.\n",
        "            feature_vector = np.add(feature_vector, model[word])\n",
        "    \n",
        "    if nwords:\n",
        "        feature_vector = np.divide(feature_vector, nwords)\n",
        "        \n",
        "    return feature_vector\n",
        "    \n",
        "def averaged_word_vectorizer_ft(corpus, model, num_features):\n",
        "    vocabulary = set(model.wv.index2word)\n",
        "    if(any(isinstance(i, list) for i in corpus)):\n",
        "      features = [average_word_vectors_ft(tokenized_sentence, model, vocabulary, num_features)\n",
        "                      for tokenized_sentence in corpus]\n",
        "      return np.array(features)\n",
        "    else:\n",
        "      features = average_word_vectors_ft(corpus, model, vocabulary, num_features)\n",
        "      return np.array(features)"
      ],
      "metadata": {
        "id": "KaASlDQlT6yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def average_word_vectors_word2vec(words, model, vocabulary, num_features):\n",
        "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
        "    nwords = 0.\n",
        "    \n",
        "    for word in words:\n",
        "        if word in vocabulary: \n",
        "            nwords = nwords + 1.\n",
        "            feature_vector = np.add(feature_vector, model[word])\n",
        "    \n",
        "    if nwords:\n",
        "        feature_vector = np.divide(feature_vector, nwords)\n",
        "        \n",
        "    return feature_vector\n",
        "\n",
        "def averaged_word_vectorizer_word2vec(corpus, model, num_features):\n",
        "    vocabulary = set(model.wv.vocab)\n",
        "    if(any(isinstance(i, list) for i in corpus)):\n",
        "      features = [average_word_vectors_word2vec(tokenized_sentence, model, vocabulary, num_features)\n",
        "                      for tokenized_sentence in corpus]\n",
        "      return np.array(features)\n",
        "    else:\n",
        "      features = average_word_vectors_word2vec(corpus, model, vocabulary, num_features)\n",
        "      return np.array(features)"
      ],
      "metadata": {
        "id": "HjCjMJR2UHoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def average_word_vectors_glove(words, model, vocabulary, num_features):  \n",
        "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
        "    nwords = 0.  \n",
        "\n",
        "    for word in words:\n",
        "        if word in vocabulary: \n",
        "            nwords = nwords + 1.\n",
        "            feature_vector = np.add(feature_vector, model.word_vectors[model.dictionary[word]])\n",
        "\n",
        "    if nwords:\n",
        "        feature_vector = np.divide(feature_vector, nwords)\n",
        "        \n",
        "    return feature_vector\n",
        "    \n",
        "\n",
        "\n",
        "def averaged_word_vectorizer_glove(corpus, model, num_features):\n",
        "    vocabulary = set(model.dictionary)\n",
        "    if(any(isinstance(i, list) for i in corpus)):\n",
        "      features = [average_word_vectors_glove(tokenized_sentence, model, vocabulary, num_features)\n",
        "                      for tokenized_sentence in corpus]\n",
        "      return np.array(features)\n",
        "    else:\n",
        "      features = average_word_vectors_glove(corpus, model, vocabulary, num_features)\n",
        "      return np.array(features)"
      ],
      "metadata": {
        "id": "j3wzG4tVTzSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sim(vec1, vec2): \n",
        "  sim1 = 1/(1+distance.euclidean(vec1, vec2))\n",
        "  sim2 = cosine_similarity(vec1, vec2)\n",
        "  sim=(sim1+sim2)/2 \n",
        "  return sim"
      ],
      "metadata": {
        "id": "zBCKddUgUQjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fusion3(vec1, vec2, vec3, vec4, vec5, vec6, fusion_no):\n",
        "    if (fusion_no == '4'):\n",
        "        master = np.concatenate((vec1, vec2, vec3), axis=1)\n",
        "        pca = PCA(n_components=100)\n",
        "        master = pca.fit_transform(master)\n",
        "        vec_duplicate = np.concatenate((vec4, vec5, vec6), axis=0)\n",
        "        vec_duplicate=[vec_duplicate]\n",
        "        vec_duplicate = pca.transform(vec_duplicate)\n",
        "        return vec_duplicate, master\n",
        "\n",
        "    else:\n",
        "        raise ValueError('Invalid value for fusion')"
      ],
      "metadata": {
        "id": "RxOXMUMkUZsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def feature_vectors_multi_modality(DR, corpus, model1, model2, model3, fusion_no):\n",
        "\n",
        "    master_ft = averaged_word_vectorizer_ft(corpus=sent, model=model1, num_features=100)\n",
        "    master_glove = averaged_word_vectorizer_glove(corpus=sent, model=model2, num_features=100)\n",
        "    master_w2v = averaged_word_vectorizer_word2vec(corpus=sent, model=model3, num_features=100)\n",
        "\n",
        "    vec_duplicate = averaged_word_vectorizer_ft(corpus=DR, model=model1, num_features=100)\n",
        "    vec_duplicate = averaged_word_vectorizer_glove(corpus=DR, model=model2, num_features=100)\n",
        "    vec_duplicate = averaged_word_vectorizer_word2vec(corpus=DR, model=model3, num_features=100)\n",
        "\n",
        "    vec_duplicate , master = fusion3(master_ft,  master_glove, master_w2v, vec_duplicate,  vec_duplicate, vec_duplicate, fusion_no)\n",
        "    \n",
        "\n",
        "    return vec_duplicate,master"
      ],
      "metadata": {
        "id": "3GXSKzbiUjs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = '/content/drive/MyDrive/result_reports.csv'\n",
        "df = pd.read_csv(url)\n",
        "df"
      ],
      "metadata": {
        "id": "F_ZD9pp3F5Fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_upCThjJV-RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_topn(model1, model2, model3, sent, DR, topn, modal, fusion_no):\n",
        "    \n",
        "    similarity=[]\n",
        "\n",
        "    if (modal == 'multi'):\n",
        "        vec_duplicate, master= feature_vectors_multi_modality(DR, sent, model1, model2, model3, fusion_no)\n",
        "    else:\n",
        "        raise ValueError('Invalid Modality entered')\n",
        "\n",
        "    for doc in range(len(master)):\n",
        "        vec_master = master[doc]\n",
        "        vec_master= [vec_master]\n",
        "        unified_sim = sim(vec_duplicate, vec_master)\n",
        "\n",
        "        similarity.append(unified_sim)\n",
        "\n",
        "    similarity = np.asarray(similarity)\n",
        "    similarity= np.concatenate(similarity, axis=0 )\n",
        "    similarity= np.concatenate(similarity, axis=0 )\n",
        "    max_similar_reports=similarity.argsort()[-topn:][::-1]\n",
        "\n",
        "    return(max_similar_reports)"
      ],
      "metadata": {
        "id": "PGOs4p1TUp80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for mod in range(10):\n",
        "  exec('w2vmodel{} = Word2Vec.load(\"/content/drive/MyDrive/word2vec_ldamallet{}.model\")'.format(mod, mod))\n",
        "  exec('ftmodel{} = FastText.load(\"/content/drive/MyDrive/ftmodel_ldamallet{}.model\")'.format(mod, mod))\n",
        "  exec('glove{} = Glove.load(\"/content/drive/MyDrive/glove_ldamallet{}.model\")'.format(mod, mod))"
      ],
      "metadata": {
        "id": "hcBSRp--x-eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recommendations(max_cluster, sample):\n",
        "        for max in max_cluster:             \n",
        "            exec('model1 = FastText.load(\"/content/drive/MyDrive/ftmodel_ldamallet{}.model\")'.format(max))              \n",
        "            exec('model2 = Glove.load(\"/content/drive/MyDrive/glove_ldamallet{}.model\")'.format(max))   \n",
        "            exec('model3 =  Word2Vec.load(\"/content/drive/MyDrive/word2vec_ldamallet{}.model\")'.format(max))                \n",
        "            exec('sent = topic{}.Description'.format(max)) \n",
        "            exec('cluster = topic{}'.format(max))  \n",
        "\n",
        "            topn = 10                                          \n",
        "            fusion_no = '4'   \n",
        "            modal = 'multi'\n",
        "            max_sim = compare_topn(model1, model2, model3, sent, sample, topn, modal, fusion_no)  \n",
        "            max_sim = max_sim.tolist()\n",
        "\n",
        "        return max_sim "
      ],
      "metadata": {
        "id": "OpteE9FKVwc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "\n",
        "ldamallet = pickle.load(open(\"/content/drive/MyDrive/ldamallet.pkl\", \"rb\"))\n",
        "dictionary = pickle.load(open(\"/content/drive/MyDrive/dictionary.pickle\", \"rb\"))\n",
        "\n",
        "@app.route(\"/\", methods =[\"GET\", \"POST\"])\n",
        "def get_recommendation():\n",
        "\n",
        "    if request.method == \"POST\":\n",
        "       \n",
        "        bug_report = request.form.get(\"bug_report\")\n",
        "        sample = cleanTextFromDescriptionAndTitle(bug_report)\n",
        "        sample = wordTokenizationAndLemmatization(sample)\n",
        "        max_cluster = sim_with_clusters_lda_topn(sample, 1)\n",
        "        max_sim = recommendations(max_cluster, sample)\n",
        "        return jsonify(max_sim)\n",
        "    return render_template(\"index.html\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()\n"
      ],
      "metadata": {
        "id": "mNJAsaLYsC8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WUYNDd7osC-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7nAq5HZVsDCb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1wikVEsG5Du7VgfmpcFObe3NA0FJtKWWv",
      "authorship_tag": "ABX9TyOBZ1kGOYO5yXfZ8QwYShow",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}