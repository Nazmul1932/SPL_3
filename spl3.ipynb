{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nazmul1932/SPL_3/blob/main/spl3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "id": "XYTh6R8J-XkE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3000510-30c5-40ca-deb9-24ca5aa9f94b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aboFd-yUs61z"
      },
      "outputs": [],
      "source": [
        "!python -m pip install --upgrade pip\n",
        "!pip install pyLDAvis\n",
        "# import pyLDAvis\n",
        "import pyLDAvis.gensim_models # don't skip this\n",
        "\n",
        "from pprint import pprint\n",
        "import time\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from gensim.models import CoherenceModel\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess as SIMPLE_PREPROCESS  \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.parsing.preprocessing import STOPWORDS as STOP_WORDS\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "from gensim import corpora,models\n",
        "import time\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "import collections\n",
        "\n",
        "try:\n",
        "    collectionsAbc = collections.abc\n",
        "except AttributeError:\n",
        "    collectionsAbc = collections \n",
        "try:\n",
        "    from collections.abc import Callable  # noqa\n",
        "except ImportError:\n",
        "    from collections import Callable  # noqa\n",
        "\n",
        "\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from sklearn.cluster import MiniBatchKMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Y8tx6cheKjQx"
      },
      "outputs": [],
      "source": [
        "pd.options.display.max_colwidth = 1000\n",
        "pd.options.display.max_rows = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90QWAv8MjYZB"
      },
      "outputs": [],
      "source": [
        "csv_file = \"/content/drive/MyDrive/eclipse_platform.csv\"\n",
        "\n",
        "bug_report = pd.read_csv(csv_file)\n",
        "bug_report.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbIUBcdYjiqJ"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "bug_report = bug_report.drop(columns=['Component', 'Status', 'Resolution', 'Version', 'Created_time', 'Resolved_time'])\n",
        "bug_report = bug_report.dropna(axis=0, subset=['Description'])\n",
        "bug_report[bug_report['Description'].map(len) > 4]\n",
        "\n",
        "bug_report.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "haOUMetGEADH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69d88921-c7ef-4b8d-eae3-02e86b47885a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 364 ms, sys: 0 ns, total: 364 ms\n",
            "Wall time: 367 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "bug_report[\"Description\"] = bug_report[\"Description\"].str.replace(\"Fixed in HEAD\", \"\")\n",
        "bug_report[\"Description\"] = bug_report[\"Description\"].str.replace(\"Has been marked as read-only\", \" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLMvbsybJpve"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "def cleanTextFromDescriptionAndTitle(report):\n",
        "\n",
        "  report = report.replace('\\t', '')\n",
        "  report = re.sub('\\w*\\d\\w*', '', report)\n",
        "  report = report.lower()\n",
        "  report = report.translate(str.maketrans('', '', string.punctuation))\n",
        "    \n",
        "  return report\n",
        "\n",
        "bug_report['Description'] = bug_report['Description'].apply(lambda x: cleanTextFromDescriptionAndTitle(x))\n",
        "bug_report['Title'] = bug_report['Title'].apply(lambda x: cleanTextFromDescriptionAndTitle(x))\n",
        "\n",
        "bug_report.loc[[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCVZf0j5NfOu"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "def wordTokenizationAndLemmatization(report):\n",
        "\n",
        "    new_report = []\n",
        "    for token in SIMPLE_PREPROCESS(report):\n",
        "        if token not in STOP_WORDS:\n",
        "          if len(token) > 5:\n",
        "            new_report.append(WordNetLemmatizer().lemmatize(token, pos='v'))\n",
        "    return new_report\n",
        "\n",
        "bug_report['Title'] = bug_report['Title'].apply(lambda x: wordTokenizationAndLemmatization(x))\n",
        "bug_report['Description'] = bug_report['Description'].apply(lambda x: wordTokenizationAndLemmatization(x))\n",
        "\n",
        "bug_report.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GG56OFYO1Wx"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "duplicated_reports = bug_report.dropna(axis=0, subset=['Duplicated_issue'])\n",
        "duplicated_reports.reset_index(drop=True, inplace=True)\n",
        "duplicated_reports.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NI8-J1wIRfFu"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "master_reports = bug_report[bug_report.isnull().any(axis=1)]\n",
        "master_reports.reset_index(drop=True, inplace=True)\n",
        "master_reports.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhFgIK5ll1vN"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "dictionary = gensim.corpora.Dictionary(master_reports['Description'])\n",
        "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
        "\n",
        "count = 0\n",
        "for key, value in dictionary.iteritems():\n",
        "  print(key, value)\n",
        "  count += 1\n",
        "  if count > 100:\n",
        "      break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGdCkvESbmUO"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "bag_of_words_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in master_reports['Description']]\n",
        "bag_of_words_corpus[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flK_bC9Tcn8w"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "bag_of_words_doc = bag_of_words_corpus[0]\n",
        "\n",
        "for i in range(len(bag_of_words_doc)):\n",
        "    print(\"{}. \\\"{}\\\" word seen {} time.\".format(i+1, dictionary[bag_of_words_doc[i][0]], bag_of_words_doc[i][1]))\n",
        "                                               "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChXCgWEbky4d"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "corpus = bag_of_words_corpus\n",
        "\n",
        "lda_model = gensim.models.LdaMulticore(corpus, num_topics=10, id2word=dictionary, passes=20, workers=2, iterations=100)\n",
        "\n",
        "pprint(lda_model.print_topics())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('drive/My Drive/dictionary.pickle', 'wb')\n",
        "\n",
        "# dump information to that file\n",
        "pickle.dump(dictionary, file)"
      ],
      "metadata": {
        "id": "_0rc-Hgr7gIg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f= open('drive/My Drive/bow_corpus.pickle', 'wb')\n",
        "\n",
        "# dump information to that file\n",
        "pickle.dump(bag_of_words_corpus, f)"
      ],
      "metadata": {
        "id": "rn3mtb-67o0u"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model.save('drive/My Drive/lda_model.model')"
      ],
      "metadata": {
        "id": "D3Dyy9GV7gcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27qLz3BEqOX9"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "print('\\nPerplexity: ', lda_model.log_perplexity(bag_of_words_corpus))\n",
        "\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=bug_report['Description'], dictionary=dictionary, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim_models.prepare(lda_model, bag_of_words_corpus, dictionary)\n",
        "vis"
      ],
      "metadata": {
        "id": "HqHW1B1A-ayd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRFgJ1k5AxlL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69b11411-1aa5-4637-8e89-a84abb3e42a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 0.5446290969848633\t Topic: 0.018*\"target\" + 0.017*\"extension\" + 0.015*\"support\" + 0.014*\"change\" + 0.013*\"property\" + 0.012*\"provide\" + 0.011*\"content\" + 0.011*\"define\" + 0.010*\"default\" + 0.010*\"currently\"\n",
            "Score: 0.050613515079021454\t Topic: 0.093*\"project\" + 0.031*\"create\" + 0.027*\"workspace\" + 0.021*\"change\" + 0.018*\"package\" + 0.016*\"delete\" + 0.016*\"eclipse\" + 0.016*\"folder\" + 0.015*\"resource\" + 0.014*\"explorer\"\n",
            "Score: 0.05060430243611336\t Topic: 0.050*\"select\" + 0.035*\"dialog\" + 0.028*\"search\" + 0.028*\"action\" + 0.025*\"button\" + 0.022*\"context\" + 0.019*\"selection\" + 0.017*\"change\" + 0.015*\"compare\" + 0.014*\"filter\"\n",
            "Score: 0.05060174688696861\t Topic: 0.060*\"editor\" + 0.032*\"window\" + 0.024*\"perspective\" + 0.022*\"eclipse\" + 0.018*\"daemon\" + 0.014*\"switch\" + 0.013*\"workbench\" + 0.013*\"editors\" + 0.013*\"scroll\" + 0.013*\"open\"\n",
            "Score: 0.050595398992300034\t Topic: 0.029*\"create\" + 0.027*\"reproduce\" + 0.024*\"windows\" + 0.021*\"attachment\" + 0.019*\"attach\" + 0.018*\"problem\" + 0.017*\"browser\" + 0.017*\"display\" + 0.016*\"eclipse\" + 0.016*\"character\"\n",
            "Score: 0.05059421807527542\t Topic: 0.060*\"eclipse\" + 0.024*\"update\" + 0.022*\"version\" + 0.022*\"feature\" + 0.022*\"plugin\" + 0.011*\"platform\" + 0.010*\"instal\" + 0.010*\"install\" + 0.010*\"plugins\" + 0.009*\"problem\"\n",
            "Score: 0.05059080943465233\t Topic: 0.054*\"public\" + 0.044*\"import\" + 0.036*\"return\" + 0.031*\"display\" + 0.023*\"string\" + 0.022*\"object\" + 0.019*\"method\" + 0.017*\"static\" + 0.013*\"listener\" + 0.011*\"private\"\n",
            "Score: 0.05059031397104263\t Topic: 0.078*\"launch\" + 0.049*\"libjvmdylib\" + 0.034*\"console\" + 0.030*\"libsystembdylib\" + 0.029*\"libclientdylib\" + 0.019*\"memory\" + 0.019*\"configuration\" + 0.016*\"background\" + 0.012*\"thread\" + 0.012*\"output\"\n",
            "Score: 0.050590287894010544\t Topic: 0.072*\"thread\" + 0.046*\"method\" + 0.035*\"javathread\" + 0.029*\"threadblocked\" + 0.022*\"native\" + 0.021*\"wait\" + 0.019*\"lock\" + 0.017*\"available\" + 0.017*\"cwindowsdll\" + 0.016*\"worker\"\n",
            "Score: 0.050590287894010544\t Topic: 0.101*\"source\" + 0.076*\"message\" + 0.072*\"method\" + 0.033*\"bundle\" + 0.031*\"exception\" + 0.018*\"occur\" + 0.017*\"resolve\" + 0.016*\"orgeclipseosgi\" + 0.016*\"fail\" + 0.016*\"follow\"\n"
          ]
        }
      ],
      "source": [
        "unseen_document = \"setup a project that contains a gif resource release project to cvs edit the gif resource with an external editor eg paintshop save and close external editor in navigator open the icon resource and verify that your changes are there release project nothing to release in navigator open the icon resource and verify that your changes are still there problem because i never refreshed from local the workspace hasnt changed so release didnt find anything however opening the resource with.\"\n",
        "\n",
        "\n",
        "bow_vector = topic=lda_model[dictionary.doc2bow(wordTokenizationAndLemmatization(unseen_document))]\n",
        "\n",
        "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
        "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 10)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "for c in range(10):\n",
        "    exec('topic_{} = pd.DataFrame()'.format(c))\n",
        "    for i in range(len(master_reports)):\n",
        "        topic=lda_model[dictionary.doc2bow(master_reports.Description[i])]\n",
        "        topic= np.asarray(topic)\n",
        "        if int(topic[np.argmax(topic[:,1]),0])== c:\n",
        "            exec('topic_{} = topic_{}.append(master_reports.loc[[i]])'.format(c,c))\n",
        "            exec('topic_{} = topic_{}.reset_index(drop=True)'.format(c,c))\n",
        "            exec('topic_{}.to_csv(\"topic_{}.csv\")'.format(c,c))\n"
      ],
      "metadata": {
        "id": "XPzbhsN5-Xes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "for c in range(10):\n",
        "  exec('topic_{} = pd.read_csv(\"topic_{}.csv\", error_bad_lines=False, engine=\"python\")'.format(c,c))\n",
        "  exec(\"topic_{}= topic_{}.drop(columns=['Unnamed: 0'])\".format(c,c))\n",
        "  exec(\"topic_{}['Description'] = topic_{}['Description'].map(wordTokenizationAndLemmatization)\".format(c,c))"
      ],
      "metadata": {
        "id": "fvGI5cB5EtHT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42b3b0fd-5190-4852-8677-c7d3fa1f2f05"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 11.2 s, sys: 208 ms, total: 11.4 s\n",
            "Wall time: 13 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "for i in range(10):\n",
        "  exec('sent_{} = []'.format(i))\n",
        "  exec('x = topic_{}'.format(i))\n",
        "  for j in range(len(x)):\n",
        "    exec('sent_{}.append(topic_{}.Description[{}])'.format(i,i,j))"
      ],
      "metadata": {
        "id": "_oWRctuPEtRh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc2a0c0f-23d2-4a8c-dd16-b931b5d36ac4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.14 s, sys: 3.92 ms, total: 1.14 s\n",
            "Wall time: 1.15 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "#Length of all the cluster\n",
        "for sent in range(10):\n",
        "  exec('print(len(sent_{}))'.format(sent))\n",
        "  "
      ],
      "metadata": {
        "id": "GCU0HmnvEu83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install glove-python-binary"
      ],
      "metadata": {
        "id": "Z3sqWvNMF0nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec,FastText\n",
        "from gensim.test.utils import get_tmpfile\n",
        "from glove import Glove, Corpus\n",
        "import glove"
      ],
      "metadata": {
        "id": "dmOr5-jBF6a0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "for cluster in range(10):\n",
        "  #Preparing parameters for model training\n",
        "  exec('corpus = sent_{}'.format(cluster))    # Path to a corpus file\n",
        "  vector_size = 100                           # Dimensionality of the word vectors\n",
        "  w = 6                                       #Maximum distance between the current and predicted word within a sentence\n",
        "  min_count = 5                               # Ignores all words with total frequency lower than this.\n",
        "  CBoW = 0                                    # Training algorithm: 1 for skip-gram; otherwise CBOW(The model tries to predict the target word by trying to understand the context of the surrounding words.).\n",
        "  iterations = 100\n",
        "\n",
        "  exec('w2vmodel{} = Word2Vec(corpus, size=vector_size, window=w, min_count=min_count, sg=CBoW, iter=iterations)'.format(cluster))\n",
        "\n",
        "  exec('path = get_tmpfile(\"word2vec{}.model\")'.format(cluster))\n",
        "  exec('w2vmodel{}.save(\"word2vec{}.model\")'.format(cluster, cluster))"
      ],
      "metadata": {
        "id": "nBX9m2-dJkHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "for cluster in range(10):\n",
        "  #Preparing parameters for model training\n",
        "  exec('corpus = sent_{}'.format(cluster))\n",
        "  vector_size = 100\n",
        "  w = 6\n",
        "  min_count = 5\n",
        "  CBoW = 0\n",
        "  iterations = 100\n",
        "\n",
        "  #Training FastText model for each cluster\n",
        "  exec ('ftmodel{} = FastText(corpus, size=vector_size, window=w, min_count=min_count, sg=CBoW, iter=iterations)'.format(cluster))\n",
        "\n",
        "  #Save the all the models in individual file\n",
        "  exec('path = get_tmpfile(\"ftmodel{}.model\")'.format(cluster))\n",
        "  exec('ftmodel{}.save(\"ftmodel{}.model\")'.format(cluster, cluster))"
      ],
      "metadata": {
        "id": "3HXkmcU6P7Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "for cluster in range(10):\n",
        "  vector_size = 100\n",
        "  exec('glove_corpus{}=Corpus()'.format(cluster, cluster)) \n",
        "  exec('glove_corpus{}.fit(sent_{})'.format(cluster, cluster))\n",
        "  exec('glove{}= Glove(no_components=vector_size, learning_rate=0.18, alpha=0.75, max_count=100, max_loss=10.0, random_state=None)'.format(cluster, cluster))\n",
        "  exec('glove{}.fit(glove_corpus{}.matrix, epochs=1000, no_threads=3, verbose=True)'.format(cluster, cluster))\n",
        "  exec('transformer = lambda dictionary2:glove{}.transform_paragraph(words, epochs=1000,ignore_missing=False)'.format(cluster, cluster))\n",
        "  exec('glove{}.add_dictionary(glove_corpus{}.dictionary)'.format(cluster, cluster))\n",
        "\n",
        "  #Save the all the models in individual file\n",
        "  exec('path = get_tmpfile(\"glove{}.model\")'.format(cluster))\n",
        "  exec('glove{}.save(\"glove{}.model\")'.format(cluster, cluster))"
      ],
      "metadata": {
        "id": "pKUt-5j0PZXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.spatial import distance\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "from glove import Glove, Corpus"
      ],
      "metadata": {
        "id": "98vfxCOR6KEE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('drive/My Drive/bow_corpus.pickle', 'rb')\n",
        "bow_corpus=pickle.load(f)\n",
        "\n",
        "file = open('drive/My Drive/dictionary.pickle', 'rb')\n",
        "dictionary=pickle.load(file)\n",
        "\n",
        "# later on, load trained model from file\n",
        "lda_model =  models.LdaModel.load('drive/My Drive/lda_model.model')"
      ],
      "metadata": {
        "id": "e0Lbx9bs7xYe"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = duplicated_reports\n",
        "test.head()"
      ],
      "metadata": {
        "id": "dC1jZeek8icQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for mod in range(10):\n",
        "  #import all the trained Word2Vec models\n",
        "  exec('w2vmodel{} = Word2Vec.load(\"word2vec{}.model\")'.format(mod, mod))\n",
        "\n",
        "  #import all the trained FastText models\n",
        "  exec('ftmodel{} = FastText.load(\"ftmodel{}.model\")'.format(mod, mod))\n",
        "\n",
        "  #import all the trained GloVe models\n",
        "  exec('glove{} = Glove.load(\"glove{}.model\")'.format(mod, mod))"
      ],
      "metadata": {
        "id": "tiGu4uEJBnEE"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sim_with_clusters_lda_topn(DR, n):\n",
        "    vec_bow = dictionary.doc2bow(DR)\n",
        "    x= lda_model[vec_bow]\n",
        "    topic = np.asarray(x)\n",
        "    # max_sim = int(topic[np.argmax(topic[:,1]),0]) \n",
        "    # max_sim\n",
        "    sim=[]\n",
        "    x= topic[np.argsort(topic[:,1])[-n:][::-1],0]\n",
        "    for i in range(len(x)):\n",
        "        sim.append(int(x[i]))\n",
        "    # return max_sim\n",
        "    return sim\n"
      ],
      "metadata": {
        "id": "2-2O22UzCTY8"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def average_word_vectors_glove(words, model, vocabulary, num_features):  \n",
        "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
        "    nwords = 0.  \n",
        "\n",
        "    for word in words:\n",
        "        if word in vocabulary: \n",
        "            nwords = nwords + 1.\n",
        "            feature_vector = np.add(feature_vector, model.word_vectors[model.dictionary[word]])\n",
        "\n",
        "    if nwords:\n",
        "        feature_vector = np.divide(feature_vector, nwords)\n",
        "        \n",
        "    return feature_vector\n",
        "    \n",
        "\n",
        "\n",
        "def averaged_word_vectorizer_glove(corpus, model, num_features):\n",
        "    vocabulary = set(model.dictionary)\n",
        "    if(any(isinstance(i, list) for i in corpus)):\n",
        "      features = [average_word_vectors_glove(tokenized_sentence, model, vocabulary, num_features)\n",
        "                      for tokenized_sentence in corpus]\n",
        "      return np.array(features)\n",
        "    else:\n",
        "      features = average_word_vectors_glove(corpus, model, vocabulary, num_features)\n",
        "      return np.array(features)\n"
      ],
      "metadata": {
        "id": "9L-exeNIMYDR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def average_word_vectors_w2v(words, model, vocabulary, num_features):\n",
        "    \n",
        "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
        "    nwords = 0.\n",
        "    \n",
        "    for word in words:\n",
        "        if word in vocabulary: \n",
        "            nwords = nwords + 1.\n",
        "            feature_vector = np.add(feature_vector, model[word])\n",
        "    \n",
        "    if nwords:\n",
        "        feature_vector = np.divide(feature_vector, nwords)\n",
        "        \n",
        "    return feature_vector\n",
        "    \n",
        "def averaged_word_vectorizer_w2v(corpus, model, num_features):\n",
        "    vocabulary = set(model.wv.index2word)\n",
        "    if(any(isinstance(i, list) for i in corpus)):\n",
        "      features = [average_word_vectors_w2v(tokenized_sentence, model, vocabulary, num_features)\n",
        "                      for tokenized_sentence in corpus]\n",
        "      return np.array(features)\n",
        "    else:\n",
        "      features = average_word_vectors_w2v(corpus, model, vocabulary, num_features)\n",
        "      return np.array(features)"
      ],
      "metadata": {
        "id": "bU4WPJWpM54e"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sim(vec1, vec2): \n",
        "  sim1 = 1/(1+distance.euclidean(vec1, vec2))\n",
        "  sim2 = cosine_similarity(vec1, vec2)\n",
        "  sim=(sim1+sim2)/2 \n",
        "  return sim"
      ],
      "metadata": {
        "id": "fmnJtYmNM9eD"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fusion(vec1, vec2, vec3, vec4, fusion_no):\n",
        "    if (fusion_no == '4'):\n",
        "        vec3 = [vec3]\n",
        "        vec4 = [vec4]\n",
        "        avg1 = (np.add(vec1, vec2))/2\n",
        "        pca = PCA(n_components=100)\n",
        "        avg_fit = pca.fit(avg1)\n",
        "        master = pca.transform(avg1)\n",
        "        avg2 = (np.add(vec3, vec4))/2\n",
        "        vec_duplicate = pca.transform(avg2)\n",
        "        return vec_duplicate, master\n",
        "\n",
        "    else:\n",
        "        raise ValueError('Invalid value for fusion')"
      ],
      "metadata": {
        "id": "LnMBbuHtNCAz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creation of feature vectors by multimodality feature extraction\n",
        "def feature_vectors_multi_modality(DR, corpus, model1, model2, fusion_no):\n",
        "  master_ft1 = averaged_word_vectorizer_w2v(corpus=sent, model=model1, num_features=100)\n",
        "  master_glove2 = averaged_word_vectorizer_glove(corpus=sent, model=model2, num_features=100)\n",
        "\n",
        "  vec_duplicate1 = averaged_word_vectorizer_w2v(corpus=DR, model=model1, num_features=100)\n",
        "  vec_duplicate2 = averaged_word_vectorizer_glove(corpus=DR, model=model2, num_features=100)\n",
        "\n",
        "  vec_duplicate , master= fusion(master_ft1, master_glove2, vec_duplicate1, vec_duplicate2, fusion_no)\n",
        "\n",
        "  return vec_duplicate,master"
      ],
      "metadata": {
        "id": "3m_-gQpUNDmE"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_topn(model1, model2, cluster, sent, DR, topn, modal, fusion_no):\n",
        "  similarity=[]\n",
        "\n",
        "  if (modal == 'multi'):\n",
        "  #create feature vectors for duplicate and master reports using multimodality\n",
        "    vec_duplicate, master= feature_vectors_multi_modality(DR, sent, model1, model2, fusion_no)\n",
        "  else:\n",
        "    raise ValueError('Invalid Modality entered')\n",
        "\n",
        "  for doc in range(len(master)):\n",
        "    vec_master = master[doc]\n",
        "    vec_master= [vec_master]\n",
        "    unified_sim = sim(vec_duplicate, vec_master)\n",
        "\n",
        "    similarity.append(unified_sim)\n",
        "  similarity = np.asarray(similarity)\n",
        "  similarity= np.concatenate(similarity, axis=0 )\n",
        "  similarity= np.concatenate(similarity, axis=0 )\n",
        "  max_similar_reports=similarity.argsort()[-topn:][::-1]\n",
        "#   print(max_similar_reports)\n",
        "\n",
        "#   for d,f in enumerate(max_similar_reports):\n",
        "#     #   similar_reports= similar_reports.append(cluster.loc[[f]])\n",
        "#     print(cluster.Description[[f]])\n",
        "\n",
        "  return(max_similar_reports)"
      ],
      "metadata": {
        "id": "3QHPmr7yNMlj"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recall Rate for Top-2.5K reports (Because Top-N where N = n * topn (2.5K = 3*833)) \n",
        "vec_acc=[]\n",
        "t1 = time.time()\n",
        "no_of_test_samples= int(200)\n",
        "for i in range(no_of_test_samples):\n",
        "  sample = test.Description[i] #The test sample (duplicate report)\n",
        "  n = 3\n",
        "  max_cluster = sim_with_clusters_lda_topn(sample, n)\n",
        "  v=[]\n",
        "  print(i)\n",
        "  for max in max_cluster:\n",
        "    exec('cluster = topic_{}'.format(max))              #The predicted cluster\n",
        "    exec('model1 = ftmodel{}'.format(max))              #The trained FastText model for the predicted cluster   (can be changed to other model as well viz. glove or word2vec)\n",
        "    exec('model2 = glove{}'.format(max))                #The trained Word2Vec model for the predicted cluster   (Doesn't count if using single modality)\n",
        "    exec('sent = topic_{}.Description'.format(max))     #The vocabulary for the predicted cluster\n",
        "    topn = 833                                          #The number of predicted master report for single predicted cluster\n",
        "    fusion_no = '4'   #Doesn't count if single modality #The selection of fusion used to fuse the word embeddings of two different models  (4 gives the best results)\n",
        "    modal = 'multi'                                    #Whether you want to use single feature extraction model or multi model ( for single, it'll consider just model1)\n",
        "    #This will return the Top-N predicted master reports\n",
        "    max_sim = compare_topn(model1, model2, cluster, sent, sample, topn, modal, fusion_no)\n",
        "    t2 = time.time()\n",
        "\n",
        "    # print(cluster.Description[1])\n",
        "    # print(test.Description[1])\n",
        "    #Comparing the predicted value to the ground truth\n",
        "    for num in max_sim:\n",
        "    #   print(cluster.Issue_id[num])\n",
        "    #   print(test.Duplicated_issue[i])\n",
        "      if (cluster.Issue_id[num] == test.Duplicated_issue[i]):\n",
        "          v.append(\"1\")\n",
        "      else:\n",
        "          v.append(\"0\")\n",
        "\n",
        "        \n",
        "  \n",
        "  if(all(x==v[0] for x in v)):\n",
        "    vec_acc.append(\"0\")\n",
        "  else:\n",
        "    vec_acc.append(\"1\")\n",
        "  \n",
        "\n",
        "\n",
        "#Evaluating the performance by Recall Rate\n",
        "sum = 0\n",
        "for i,num in enumerate(vec_acc):\n",
        "    sum = sum + int(num)\n",
        "recall_rate = (sum/len(vec_acc))*100\n",
        "print(\"Recall Rate : {} %\".format(recall_rate))\n",
        "print(\"Time : \", (t2-t1)/60, \"min\")"
      ],
      "metadata": {
        "id": "rvEH4T1XPJ0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XYrzHztOPJ3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fjc4pEhtPJ8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "APDt-Jz2PJ_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import sys\n",
        "from time import time\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "vectorizer = TfidfVectorizer(strip_accents='unicode', stop_words='english', min_df=2) ## Corpus is in English\n",
        "X = vectorizer.fit_transform(master_reports.Description)\n",
        "print(X.shape)\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# Sum_of_squared_distances = []\n",
        "# K = range(2,10)\n",
        "# for k in K:\n",
        "#    km = KMeans(n_clusters=k, max_iter=200, n_init=10)\n",
        "#    km = km.fit(X)\n",
        "#    Sum_of_squared_distances.append(km.inertia_)\n",
        "# plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
        "# plt.xlabel('k')\n",
        "# plt.ylabel('Sum_of_squared_distances')\n",
        "# plt.title('Elbow Method For Optimal k')\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "\n",
        "km = KMeans(n_clusters=10, init='k-means++', max_iter=100)\n",
        "t0 = time()\n",
        "km.fit(X)\n",
        "print(\"done in %0.3fs\" % (time() - t0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHL_7lz_SxzM",
        "outputId": "77ccb1a1-d6a4-422a-8e11-9bb90d52060a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(70629, 59977)\n",
            "done in 48.328s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "centroids = km.cluster_centers_.argsort()[:, ::-1] ## Indices of largest centroids' entries in descending order\n",
        "terms = vectorizer.get_feature_names()\n",
        "for i in range(10):\n",
        "    print(\"Cluster %d:\" % i, end='')\n",
        "    for ind in centroids[i, :100]:\n",
        "        print(' %s' % terms[ind], end='')\n",
        "    print()"
      ],
      "metadata": {
        "id": "SaMtL5TWVFVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = vectorizer.transform([\"opening repository resource always open the default text editor and doesnt honor any mapping between resource types and editors as a result it is not possible\"])\n",
        "prediction = km.predict(Y)\n",
        "print(prediction)"
      ],
      "metadata": {
        "id": "yiGC7C-DVFdR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1fca119-749f-4fd8-c4e0-208de052fed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topics_matrix = lda_model.show_topics(formatted=True, num_words=50)\n",
        "topics_matrix = np.array(topics_matrix)\n",
        "\n",
        "topic_words = topics_matrix[:,:]\n",
        "for i in topic_words:\n",
        "    print([str(word) for word in i])\n",
        "    print()"
      ],
      "metadata": {
        "id": "c-moRl6ZbhtG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1wikVEsG5Du7VgfmpcFObe3NA0FJtKWWv",
      "authorship_tag": "ABX9TyOM6/iVwl09rmZeDoLHhhJE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}